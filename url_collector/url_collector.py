"""
    Todo
        랜덤 프록시 적용
        txt 파일에서 keyword 불러오기
        URL 수집 결과 DB 저장
"""
import re
import time
import concurrent.futures
import logging

import requests
from bs4 import BeautifulSoup
import numpy as np
from fake_useragent import UserAgent

pattern_url = r"^\/url\?q\=(.*)"
ua = UserAgent()
url_google_search = f"https://www.google.com/search?"

keyword = ["123", "aaa"]


def collect_url(query):
    lang = "lang_ko"
    num = 100
    start = 0
    tbs = 0
    safe = 0
    country = "countryKR"
    urls = []
    while True:
        urls_count = 0
        headers = {
            "User-Agent": ua.random
        }
        params = {
            "lr": lang,  # Language
            "q": query,  # Googl Search Query
            "num": num,  # Amount of pages
            "start": start,  # Start page number
            "tbs": tbs,  #
            "safe": safe,  # Kids safe search
            "cr": country  # Country
            # "filter" : 0  →  검색 필터 끄기
        }

        """
            랜덤 프록시 적용
        """

        # 요청
        response = requests.get(url_google_search, headers=headers, params=params)
        html = response.text
        soup = BeautifulSoup(html, "html.parser")

        # URL 파싱
        for href in soup.select("div>div>div>div>a"):
            match_list = re.findall(pattern_url, href['href'])
            if match_list:
                urls_count += 1
                urls.append(match_list[0])

        # 현재 페이지에 URL이 존재 한다면 추가 진행
        if urls_count != 0:
            start += 100  # 다음 페이지 검색을 위한 증가

        # 현재 페이지에 URL이 존재하지 않다면 종료
        else:
            break
    return urls


if __name__ == '__main__':
    result = []
    pool = concurrent.futures.ProcessPoolExecutor()
    procs = []

    """
        txt 파일에서 keyword 불러오기
    """
    for i in keyword:
        procs.append(pool.submit(collect_url, i))

    for p in concurrent.futures.as_completed(procs):
        result.append(p.result())

    result = np.array(result).flatten().tolist()

    """
        URL 수집 결과 DB 저장
    """